{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993f0877",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88a978ac",
   "metadata": {},
   "source": [
    "## 1. Configuração do Ambiente\n",
    "\n",
    "Importamos bibliotecas, iniciamos a `SparkSession` e registramos as tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5503101",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize Spark\u001b[39;00m\n\u001b[0;32m     10\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFatalModelAnalysis\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.files.maxPartitionBytes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m64MB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.default.parallelism\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Create SQLAlchemy engine\u001b[39;00m\n\u001b[0;32m     17\u001b[0m engine \u001b[38;5;241m=\u001b[39m create_engine(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqlite:///data/database.db\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\sql\\session.py:556\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    554\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    555\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[0;32m    557\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\core\\context.py:523\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 523\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\core\\context.py:205\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m     )\n\u001b[1;32m--> 205\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    208\u001b[0m         master,\n\u001b[0;32m    209\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    220\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\core\\context.py:444\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 444\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    445\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\java_gateway.py:108\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 108\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    112\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    113\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    114\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. Carregamento Completo e Otimizado com Pandas\n",
    "# Autora: Sophia Katze de Paula – 2025-06-11\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandasql import sqldf\n",
    "import py7zr  \n",
    "\n",
    "# Configurações gerais\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "#Unzip do Dataset\n",
    "py7zr.SevenZipFile('arquivo.7z', mode='r').extractall(path='destino/')\n",
    "\n",
    "\n",
    "# Carregar os dados (simulação de caminho para exemplo)\n",
    "df_items = pd.read_csv('../data/user_transaction_items_retificado2.csv')\n",
    "df_users = pd.read_csv('../data/users.csv')\n",
    "\n",
    "# Remover espaços dos nomes das colunas\n",
    "df_items.columns = df_items.columns.str.replace(' ', '_')\n",
    "df_users.columns = df_users.columns.str.replace(' ', '_')\n",
    "\n",
    "# Exibir os novos nomes das colunas\n",
    "print(\"Colunas de df_items:\")\n",
    "print(df_items.columns.tolist())\n",
    "\n",
    "print(\"\\nColunas de df_users:\")\n",
    "print(df_users.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd61222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data using SQLAlchemy and convert to Spark dataframes\n",
    "users_pd = pd.read_sql_table('users', engine)\n",
    "transactions_pd = pd.read_sql_table('user_transactions', engine)\n",
    "\n",
    "users_df = spark.createDataFrame(users_pd)\n",
    "transactions_df = spark.createDataFrame(transactions_pd)\n",
    "\n",
    "# Cache the dataframes\n",
    "users_df.cache()\n",
    "transactions_df.cache()\n",
    "\n",
    "# Create temporary views\n",
    "users_df.createOrReplaceTempView(\"users\")\n",
    "transactions_df.createOrReplaceTempView(\"user_transactions\")\n",
    "\n",
    "files_to_load = [\n",
    "    (r\"data\\users.csv\", \"users\"),\n",
    "    (r\"data\\user_transaction_items_retificado2.csv\", \"user_transactions\")\n",
    "]\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    dataframes = dict(zip(\n",
    "        [f[1] for f in files_to_load],\n",
    "        executor.map(lambda x: spark.createDataFrame(pd.read_sql_table(x[1], engine)), files_to_load)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9841c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para executar SQL nos DataFrames\n",
    "def run_sql(path):\n",
    "    query = open(path).read()\n",
    "    return spark.sql(query, globals=dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7816a996",
   "metadata": {},
   "source": [
    "## 2. Desafio 1.1: Taxa de Usuários por Status\n",
    "\n",
    "Qual a proporção de usuários ativos, onboarding, desabilitados e deletados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55656a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_status = run_sql(r\"sql/01_user_status.sql\")\n",
    "df_status.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9938689",
   "metadata": {},
   "source": [
    "**Insight:** A saúde da base é medida por X% ativos e Y% banidos, direcionando estratégias de retenção e reengajamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a578da0",
   "metadata": {},
   "source": [
    "## 3. Desafio 1.2: Padrão de Compras e Sazonalidade\n",
    "\n",
    "Como variam as compras e receita ao longo do tempo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e22d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily = run_sql(\"sql/02_daily_sales.sql\").toPandas()\n",
    "sns.lineplot(data=df_daily, x='dt', y='revenue')\n",
    "plt.title(\"Receita Diária\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a1f1ff",
   "metadata": {},
   "source": [
    "**Insight:** Observamos sazonalidade semanal com picos em datas específicas, indicando janelas ideais para campanhas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f84bc3",
   "metadata": {},
   "source": [
    "## 4. Desafio 1.3: Usuários Pagantes e ARPU\n",
    "\n",
    "Quantos usuários pagam e quanto em média gastam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0220cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arpu = run_sql(\"sql/03_spend_per_user.sql\").toPandas()\n",
    "df_arpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ec7ee2",
   "metadata": {},
   "source": [
    "## 5. Desafio 1.4: Faturamento Mensal\n",
    "\n",
    "Qual a performance de receita mês a mês?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573b93ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly = run_sql(\"sql/04_monthly_revenue.sql\").toPandas()\n",
    "sns.barplot(data=df_monthly, x='month', y='revenue')\n",
    "plt.title(\"Receita Mensal\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83231b2f",
   "metadata": {},
   "source": [
    "## 6. Desafio 2.1: Identificação da Promoção\n",
    "\n",
    "Quando ocorreu a promoção de 85% de desconto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d28ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "periodo = run_sql(\"sql/05_promotion_period.sql\").collect()\n",
    "print(f\"Promoção de ~85% de desconto: {periodo[0][0]} até {periodo[-1][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39f39f8",
   "metadata": {},
   "source": [
    "## 7. Desafio 2.2: Impacto da Promoção\n",
    "\n",
    "Como a receita e o número de transações mudaram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ae85e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_impact = run_sql(\"sql/06_promotion_impact.sql\").toPandas()\n",
    "df_impact\n",
    "sns.barplot(data=df_impact.melt(id_vars='period', value_vars=['avg_rev','avg_tx']), x='period', y='value', hue='variable')\n",
    "plt.title(\"Impacto da Promoção\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8eb1ea",
   "metadata": {},
   "source": [
    "## 8. Desafio 2.3: Simulação Monte Carlo de Desconto Ideal\n",
    "\n",
    "Executamos a simulação para recomendar desconto ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed0a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.promo_simulation import carregar_dados_spark, identificar_periodo_promocional_spark, simulacao_monte_carlo\n",
    "\n",
    "df_trans = carregar_dados_spark(\"data/user_transactions.csv\")\n",
    "inicio, fim = identificar_periodo_promocional_spark(df_trans, limite_desconto=0.85)\n",
    "resultados = simulacao_monte_carlo(df_trans, (inicio, fim), descontos=list(range(0,91,10)), n_sim=5000)\n",
    "import pandas as pd\n",
    "df_sim = pd.DataFrame(resultados)\n",
    "sns.lineplot(data=df_sim, x='discount_pct', y='mean_revenue', marker='o')\n",
    "plt.fill_between(df_sim['discount_pct'], df_sim['revenue_5th_pct'], df_sim['revenue_95th_pct'], alpha=0.3)\n",
    "plt.title(\"Receita vs Desconto\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7d9798",
   "metadata": {},
   "source": [
    "## 9. Conclusão e Próximos Passos\n",
    "\n",
    "- Resumo dos principais insights.\n",
    "- Ações recomendadas: testes de descontos moderados, monitoramento contínuo via Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7289d5f7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
